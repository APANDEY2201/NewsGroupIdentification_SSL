{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "01_newsgroup_Preprocessingpipeline.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5X_iA6JfUuf"
      },
      "source": [
        "import sklearn\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "\n",
        "from gensim.models import Word2Vec, Phrases\n",
        "\n",
        "from sklearn import neighbors\n",
        "from sklearn import metrics\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "from sklearn import neighbors, datasets\n",
        "import re\n",
        "import string"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYF7OGwsfUuh",
        "outputId": "eb00739a-7b2b-4101-808c-71a9b483e93e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#load data\n",
        "newsgroups_train = fetch_20newsgroups(subset='train',shuffle=True, remove=('headers', 'footers', 'quotes'))\n",
        "newsgroups_test = fetch_20newsgroups(subset='test',shuffle=True, remove=('headers', 'footers', 'quotes'))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqNfC4uRfUui"
      },
      "source": [
        "def filter_data():\n",
        "    #filter data\n",
        "    # newsgroups_train = fetch_20newsgroups(subset='train',remove=('headers', 'footers', 'quotes'))\n",
        "    # newsgroups_test = fetch_20newsgroups(subset='test',remove=('headers', 'footers', 'quotes'))\n",
        "    \n",
        "    class_names = newsgroups_train.target_names\n",
        "    print('Training data size::', len(newsgroups_train['data']))\n",
        "    \n",
        "    # Printing all the categories\n",
        "    print('Training categories names::', newsgroups_train.target_names)\n",
        "    \n",
        "    # Finding frequency of each category\n",
        "    targets, frequency = np.unique(newsgroups_train.target, return_counts=True)\n",
        "    targets_str = np.array(newsgroups_train.target_names)\n",
        "    \n",
        "    #Prinnting all frequency\n",
        "    #print(\"Print targets, frequency in array::\", targets, frequency)\n",
        "    print(list(zip(targets_str, frequency)))\n",
        "    \n",
        "    #Training data class distribution\n",
        "    #fig=plt.figure(figsize=(10, 5), dpi= 80, facecolor='w', edgecolor='k')\n",
        "    #plt.bar(targets_str,frequency, color='#ff6961')\n",
        "    #plt.xticks(rotation=90)\n",
        "    #plt.title('Class distribution of 20 Newsgroups Training Data')\n",
        "    #plt.xlabel('News Group')\n",
        "    #plt.ylabel('Frequency')\n",
        "    #plt.show()\n",
        "    \n",
        "    # Preparing train data\n",
        "    newsgroups_train_df = pd.DataFrame({'data': newsgroups_train.data, 'target': newsgroups_train.target})\n",
        "\n",
        "    # Processing the data to remove partial labels\n",
        "\n",
        "    newsgroups_split_train, newsgroups_split_test = train_test_split(newsgroups_train_df,train_size = 0.3, random_state = 42, shuffle = True)\n",
        "\n",
        "    newsgroups_split_test = newsgroups_split_train.drop('target', axis = 1)\n",
        "\n",
        "    newsgroups_train_df = pd.concat([newsgroups_split_train, newsgroups_split_test], axis=0)\n",
        "\n",
        "\n",
        "    #print(newsgroups_train_df.head())\n",
        "    # Preparing test data\n",
        "    newsgroups_test_df = pd.DataFrame({'data': newsgroups_test.data, 'target': newsgroups_test.target})\n",
        "\n",
        "    newsgroups_test_df = newsgroups_test_df.drop('target',axis = 1)\n",
        "    print(newsgroups_test_df)\n",
        "    \n",
        "    # Text preprocessing steps - remove numbers, captial letters and punctuation\n",
        "    newsgroups_train_df.data.str.replace('[^a-zA-Z]', '')\n",
        "    newsgroups_test_df.data.str.replace('[^a-zA-Z]', '')\n",
        "    newsgroups_train_df = newsgroups_train_df.data.str.lower()\n",
        "    newsgroups_test_df = newsgroups_test_df.data.str.lower()\n",
        "    print(newsgroups_train_df.head())\n",
        "    \n",
        "\n",
        "    return newsgroups_train, newsgroups_test, class_names"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGJnxHtDfUuk"
      },
      "source": [
        "def bow_features(train_data, test_data):\n",
        "    # Bag-of-words representation\n",
        "    bow_vectorize = CountVectorizer()\n",
        "    bow_train = bow_vectorize.fit_transform(train_data.data) #bag-of-word features for training data\n",
        "    bow_test = bow_vectorize.transform(test_data.data)\n",
        "    feature_names = bow_vectorize.get_feature_names() #converts feature index to the word it represents.\n",
        "    shape = bow_train.shape\n",
        "    print('{} train data points.'.format(shape[0]))\n",
        "    print('{} feature dimension.'.format(shape[1]))\n",
        "    print('Most common word in training set is \"{}\"'.format(feature_names[bow_train.sum(axis=0).argmax()]))\n",
        "    return bow_train, bow_test, feature_names"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jArBOqOefUuk"
      },
      "source": [
        "def tf_idf_features(train_data, test_data):\n",
        "    # Bag-of-words representation\n",
        "    tf_idf_vectorize = TfidfVectorizer(ngram_range=(1, 3), stop_words='english')\n",
        "    tf_idf_train = tf_idf_vectorize.fit_transform(train_data.data) #bag-of-word features for training data\n",
        "    feature_names = tf_idf_vectorize.get_feature_names() #converts feature index to the word it represents.\n",
        "    tf_idf_test = tf_idf_vectorize.transform(test_data.data)\n",
        "    print(tf_idf_train)\n",
        "    return tf_idf_train, tf_idf_test, feature_names"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M823kXnvyjBe"
      },
      "source": [
        "def w2v(feature_names):\n",
        "  feature_names = list(feature_names)\n",
        "  n = 5\n",
        "  list_of_lists = [feature_names[i:i + n] for i in range(0, len(feature_names), n)]\n",
        "  print(list_of_lists)\n",
        "\n",
        "  model = Word2Vec(list_of_lists, size=300, min_count=1, workers=4)\n",
        "  # model['browser']\n",
        "  # model.similar_by_word('anarchist')\n",
        "\n",
        "  return model"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfN7_RfCfUul"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    train_data, test_data, class_names = filter_data()\n",
        "\n",
        "    # #Count Vectorization\n",
        "    # train_bow, test_bow, feature_names = bow_features(train_data, test_data)\n",
        "    \n",
        "    # # TF-idf\n",
        "    # train_bow_tf_idf, test_bow_tf_idf, feature_names_tf_idf = tf_idf_features(train_data, test_data)\n",
        "    # print('------')\n",
        "    # print(train_bow_tf_idf)\n",
        "    # print('-----')\n",
        "    # print(feature_names)\n",
        "\n",
        "    # Word2Vec\n",
        "\n",
        "    word_embedding = w2v(feature_names)\n",
        "\n",
        "    print(word_embedding.wv.similar_by_word('browser'))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}